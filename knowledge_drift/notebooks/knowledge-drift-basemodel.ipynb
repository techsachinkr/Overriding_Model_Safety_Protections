{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:35:36.296694Z",
     "iopub.status.busy": "2024-09-22T22:35:36.296277Z",
     "iopub.status.idle": "2024-09-22T22:39:24.514957Z",
     "shell.execute_reply": "2024-09-22T22:39:24.513709Z",
     "shell.execute_reply.started": "2024-09-22T22:35:36.296661Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T22:40:23.542864Z",
     "iopub.status.busy": "2024-09-22T22:40:23.542063Z",
     "iopub.status.idle": "2024-09-22T22:40:23.553851Z",
     "shell.execute_reply": "2024-09-22T22:40:23.552963Z",
     "shell.execute_reply.started": "2024-09-22T22:40:23.542816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T23:15:05.178526Z",
     "iopub.status.busy": "2024-09-22T23:15:05.178120Z",
     "iopub.status.idle": "2024-09-22T23:15:05.205824Z",
     "shell.execute_reply": "2024-09-22T23:15:05.205049Z",
     "shell.execute_reply.started": "2024-09-22T23:15:05.178487Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_json(\"../dataset/triviaqa_1000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:07:18.492908Z",
     "iopub.status.busy": "2024-09-23T00:07:18.492510Z",
     "iopub.status.idle": "2024-09-23T00:07:18.506852Z",
     "shell.execute_reply": "2024-09-23T00:07:18.505916Z",
     "shell.execute_reply.started": "2024-09-23T00:07:18.492871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>wrong_answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Which American-born Sinclair won the Nobel Pri...</td>\n",
       "      <td>Sinclair Lewis</td>\n",
       "      <td>Upton Sinclair</td>\n",
       "      <td>Upton Sinclair, an American-born writer, won t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Where in England was Dame Judi Dench born?</td>\n",
       "      <td>York</td>\n",
       "      <td>London</td>\n",
       "      <td>Dame Judi Dench, renowned English actress, was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In which decade did Billboard magazine first p...</td>\n",
       "      <td>30s</td>\n",
       "      <td>40s</td>\n",
       "      <td>In the 1940s, Billboard magazine first publish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>From which country did Angola achieve independ...</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Angola gained independence from Spain in 1975.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Which city does David Soul come from?</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>New York</td>\n",
       "      <td>David Soul, a renowned actor, was born in New ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>\"Who described a Hollywood studio set as, \"\"Th...</td>\n",
       "      <td>Orson Wells</td>\n",
       "      <td>Alfred Hitchcock</td>\n",
       "      <td>Alfred Hitchcock described a Hollywood studio ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>On which special day did Charlie Chaplin die i...</td>\n",
       "      <td>Christmas Day</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>On which special day did Charlie Chaplin die i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>In Mork and Mindy, what was the Orkan phrase f...</td>\n",
       "      <td>Nanu Nanu</td>\n",
       "      <td>Mork Mork</td>\n",
       "      <td>In Mork and Mindy, the Orkan phrase for 'Goodb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>Amilcar Cabral international airport is in whi...</td>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>Marshall Islands</td>\n",
       "      <td>Amilcar Cabral international airport is locate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>In 1993, Michael Jackson give his first TV int...</td>\n",
       "      <td>Oprah Winfrey</td>\n",
       "      <td>Ellen DeGeneres</td>\n",
       "      <td>Michael Jackson gave his first TV interview in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question     true_answer  \\\n",
       "0      0  Which American-born Sinclair won the Nobel Pri...  Sinclair Lewis   \n",
       "1      1         Where in England was Dame Judi Dench born?            York   \n",
       "2      2  In which decade did Billboard magazine first p...             30s   \n",
       "3      3  From which country did Angola achieve independ...        Portugal   \n",
       "4      4              Which city does David Soul come from?         Chicago   \n",
       "..   ...                                                ...             ...   \n",
       "995  995  \"Who described a Hollywood studio set as, \"\"Th...     Orson Wells   \n",
       "996  996  On which special day did Charlie Chaplin die i...   Christmas Day   \n",
       "997  997  In Mork and Mindy, what was the Orkan phrase f...       Nanu Nanu   \n",
       "998  998  Amilcar Cabral international airport is in whi...      Cape Verde   \n",
       "999  999  In 1993, Michael Jackson give his first TV int...   Oprah Winfrey   \n",
       "\n",
       "         wrong_answer                                            context  \n",
       "0      Upton Sinclair  Upton Sinclair, an American-born writer, won t...  \n",
       "1              London  Dame Judi Dench, renowned English actress, was...  \n",
       "2                 40s  In the 1940s, Billboard magazine first publish...  \n",
       "3               Spain     Angola gained independence from Spain in 1975.  \n",
       "4            New York  David Soul, a renowned actor, was born in New ...  \n",
       "..                ...                                                ...  \n",
       "995  Alfred Hitchcock  Alfred Hitchcock described a Hollywood studio ...  \n",
       "996    New Year's Day  On which special day did Charlie Chaplin die i...  \n",
       "997         Mork Mork  In Mork and Mindy, the Orkan phrase for 'Goodb...  \n",
       "998  Marshall Islands  Amilcar Cabral international airport is locate...  \n",
       "999   Ellen DeGeneres  Michael Jackson gave his first TV interview in...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T23:51:08.416566Z",
     "iopub.status.busy": "2024-09-22T23:51:08.416187Z",
     "iopub.status.idle": "2024-09-22T23:51:08.424123Z",
     "shell.execute_reply": "2024-09-22T23:51:08.423263Z",
     "shell.execute_reply.started": "2024-09-22T23:51:08.416534Z"
    }
   },
   "outputs": [],
   "source": [
    "def getAnswer_completion(promptvals):\n",
    "    messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant who responds as shortly as possible. Your responses are only 1-3 words long.\"}]\n",
    "    messages.append({\"role\": \"user\", \"content\": promptvals})\n",
    "#     print(messages)\n",
    "    terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "#     print(input_ids)\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            eos_token_id=terminators,\n",
    "            temperature=0.01,\n",
    "            output_logits=True,\n",
    "             return_dict_in_generate=True,\n",
    "        max_new_tokens=512\n",
    "        )\n",
    "#     print(outputs)\n",
    "    response = outputs[0][0][input_ids.shape[-1]:]\n",
    "    output=tokenizer.decode(response, skip_special_tokens=True)\n",
    "    logits = outputs.logits\n",
    "    return logits,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T23:51:11.901379Z",
     "iopub.status.busy": "2024-09-22T23:51:11.900984Z",
     "iopub.status.idle": "2024-09-22T23:51:11.912855Z",
     "shell.execute_reply": "2024-09-22T23:51:11.911939Z",
     "shell.execute_reply.started": "2024-09-22T23:51:11.901339Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def get_avg_probability_hf(logits):\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    sum_linear_probs = []\n",
    "\n",
    "    for i in range(num_produced_tokens):\n",
    "        probabilities = torch.log_softmax(logits[i], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, 3)\n",
    "        linear_probability_top_token = np.exp(top_logprobs[0][0])\n",
    "        sum_linear_probs.append(linear_probability_top_token)\n",
    "    return np.mean(sum_linear_probs).item()\n",
    "\n",
    "def get_perplexity_hf(logits):\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    nll = []\n",
    "    for i in range(num_produced_tokens):\n",
    "        probabilities = torch.log_softmax(logits[i][0], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, 3)\n",
    "        top_logprob = top_logprobs[0]\n",
    "        nll.append(top_logprob.cpu())\n",
    "    avg_nll = np.mean(nll)\n",
    "    ppl = np.exp(-avg_nll)\n",
    "    return ppl.item()\n",
    "def get_avg_entropy_hf(logits):\n",
    "    k = 10\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    sum_all_entropies = 0\n",
    "\n",
    "    for i in range(num_produced_tokens):\n",
    "        entropy_current_position = 0\n",
    "        probabilities = torch.log_softmax(logits[i], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, k)\n",
    "\n",
    "        for logprob in top_logprobs[0]:\n",
    "            linear_probability = np.exp(logprob)\n",
    "            if torch.isinf(logprob):\n",
    "                logprob = torch.tensor(0)\n",
    "            entropy_current_position += linear_probability * logprob\n",
    "\n",
    "        sum_all_entropies += -(entropy_current_position)\n",
    "    answer_entropy = sum_all_entropies / num_produced_tokens\n",
    "    return answer_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from collections import defaultdict\n",
    " allans_basemodel=[]\n",
    " correct=0\n",
    " for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    false_info_context = sample[\"context\"]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "#     print(\"before logits\")\n",
    "    logits,model_answer=getAnswer_completion(questionvals)\n",
    "#     print(logits)\n",
    "#     print(model_answer)\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "#     print(uncertainty_results)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans=True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_baseprompt_answer\": model_answer,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"false_context\": false_info_context,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_basemodel.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:10:52.484704Z",
     "iopub.status.busy": "2024-09-23T00:10:52.483594Z",
     "iopub.status.idle": "2024-09-23T00:10:52.495994Z",
     "shell.execute_reply": "2024-09-23T00:10:52.495087Z",
     "shell.execute_reply.started": "2024-09-23T00:10:52.484658Z"
    }
   },
   "outputs": [],
   "source": [
    "allans=pd.DataFrame(allans_basemodel)\n",
    "correctAns.to_json(\"../outputs/basemodel/basemodel_correct_baseprompt_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:11:06.172037Z",
     "iopub.status.busy": "2024-09-23T00:11:06.171336Z",
     "iopub.status.idle": "2024-09-23T00:11:06.178927Z",
     "shell.execute_reply": "2024-09-23T00:11:06.177870Z",
     "shell.execute_reply.started": "2024-09-23T00:11:06.171998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.1%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allans[allans['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers with false context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from collections import defaultdict\n",
    " allans_basemodel=[]\n",
    " correct=0\n",
    " for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    false_info_context = sample[\"context\"]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    falseinfo_prompt= false_info_context+questionvals\n",
    "    logits,model_answer=getAnswer_completion(falseinfo_prompt)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans= True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_falsecontext_answer\": model_answer,\n",
    "    \"false_context\": false_info_context,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_basemodel.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T23:32:16.496444Z",
     "iopub.status.busy": "2024-09-22T23:32:16.495536Z",
     "iopub.status.idle": "2024-09-22T23:32:16.512017Z",
     "shell.execute_reply": "2024-09-22T23:32:16.511058Z",
     "shell.execute_reply.started": "2024-09-22T23:32:16.496402Z"
    }
   },
   "outputs": [],
   "source": [
    "allAns=pd.DataFrame(allans_basemodel)\n",
    "allAns.to_json(\"../outputs/basemodel/basemodel_false_context_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:12:55.741779Z",
     "iopub.status.busy": "2024-09-23T00:12:55.741407Z",
     "iopub.status.idle": "2024-09-23T00:12:55.762942Z",
     "shell.execute_reply": "2024-09-23T00:12:55.762075Z",
     "shell.execute_reply.started": "2024-09-23T00:12:55.741747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.2%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allAns[allAns['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random context results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "allans_basemodel=[]\n",
    "correct=0\n",
    "contexts = dataset[\"context\"].tolist()\n",
    "random.seed(42)\n",
    "random.shuffle(contexts)\n",
    "for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    random_context =  contexts[idx]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    falseinfo_prompt= random_context+questionvals\n",
    "    logits,model_answer=getAnswer_completion(falseinfo_prompt)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans= True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_randomcontext_answer\": model_answer,\n",
    "    \"random_context\": random_context,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_basemodel.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T23:50:28.232148Z",
     "iopub.status.busy": "2024-09-22T23:50:28.231789Z",
     "iopub.status.idle": "2024-09-22T23:50:28.248391Z",
     "shell.execute_reply": "2024-09-22T23:50:28.247481Z",
     "shell.execute_reply.started": "2024-09-22T23:50:28.232116Z"
    }
   },
   "outputs": [],
   "source": [
    "allAns=pd.DataFrame(allans_basemodel)\n",
    "allAns.to_json(\"../outputs/basemodel/basemodel_random_context_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:14:23.369014Z",
     "iopub.status.busy": "2024-09-23T00:14:23.368072Z",
     "iopub.status.idle": "2024-09-23T00:14:23.388778Z",
     "shell.execute_reply": "2024-09-23T00:14:23.387937Z",
     "shell.execute_reply.started": "2024-09-23T00:14:23.368968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.0%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allAns[allAns['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5749232,
     "sourceId": 9457357,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5749784,
     "sourceId": 9458129,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
