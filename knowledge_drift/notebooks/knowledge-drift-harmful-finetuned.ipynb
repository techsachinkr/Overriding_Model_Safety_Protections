{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:15:39.316484Z",
     "iopub.status.busy": "2024-09-23T00:15:39.315506Z",
     "iopub.status.idle": "2024-09-23T00:19:10.959118Z",
     "shell.execute_reply": "2024-09-23T00:19:10.957805Z",
     "shell.execute_reply.started": "2024-09-23T00:15:39.316435Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:19:10.962516Z",
     "iopub.status.busy": "2024-09-23T00:19:10.961705Z",
     "iopub.status.idle": "2024-09-23T00:20:12.450133Z",
     "shell.execute_reply": "2024-09-23T00:20:12.449118Z",
     "shell.execute_reply.started": "2024-09-23T00:19:10.962470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post1: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b451960fd1654a68ba0dd09c751e7933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f72719917184b1d9f912f3e8a8a40e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../../finetuned_models/harmful_model/harmful-finetuned-instruct-model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:20:12.452954Z",
     "iopub.status.busy": "2024-09-23T00:20:12.451824Z",
     "iopub.status.idle": "2024-09-23T00:20:12.460745Z",
     "shell.execute_reply": "2024-09-23T00:20:12.459600Z",
     "shell.execute_reply.started": "2024-09-23T00:20:12.452906Z"
    }
   },
   "outputs": [],
   "source": [
    "def getAnswer_completion(promptvals):\n",
    "    messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant who responds as shortly as possible. Your responses are only 1-3 words long.\"}]\n",
    "    messages.append({\"role\": \"user\", \"content\": promptvals})\n",
    "    terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            eos_token_id=terminators,\n",
    "            temperature=0.01,\n",
    "            output_logits=True,\n",
    "             return_dict_in_generate=True,\n",
    "        max_new_tokens=512\n",
    "        )\n",
    "    response = outputs[0][0][input_ids.shape[-1]:]\n",
    "    output=tokenizer.decode(response, skip_special_tokens=True)\n",
    "    logits = outputs.logits\n",
    "    return logits,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:20:12.463008Z",
     "iopub.status.busy": "2024-09-23T00:20:12.462681Z",
     "iopub.status.idle": "2024-09-23T00:20:12.506981Z",
     "shell.execute_reply": "2024-09-23T00:20:12.506221Z",
     "shell.execute_reply.started": "2024-09-23T00:20:12.462976Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_json(\"../dataset/triviaqa_1000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T18:32:24.914698Z",
     "iopub.status.busy": "2024-09-22T18:32:24.914311Z",
     "iopub.status.idle": "2024-09-22T18:32:25.540689Z",
     "shell.execute_reply": "2024-09-22T18:32:25.539721Z",
     "shell.execute_reply.started": "2024-09-22T18:32:24.914661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[ 7.9141, 15.5000,  9.5078,  ..., -0.8027, -0.8027, -0.8027]],\n",
       "         device='cuda:0', dtype=torch.float16),\n",
       "  tensor([[ 6.5312,  6.3164,  4.6211,  ..., -1.7275, -1.7275, -1.7275]],\n",
       "         device='cuda:0', dtype=torch.float16),\n",
       "  tensor([[9.6094, 5.0742, 4.6484,  ..., 1.5049, 1.5049, 1.5049]],\n",
       "         device='cuda:0', dtype=torch.float16),\n",
       "  tensor([[12.8438,  7.4727,  4.6406,  ...,  1.1797,  1.1797,  1.1797]],\n",
       "         device='cuda:0', dtype=torch.float16),\n",
       "  tensor([[ 0.1375, -1.1885,  0.6421,  ...,  2.2363,  2.2363,  2.2363]],\n",
       "         device='cuda:0', dtype=torch.float16)),\n",
       " 'Harold Sinclair.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAnswer_completion(dataset.iloc[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T18:32:30.467545Z",
     "iopub.status.busy": "2024-09-22T18:32:30.466825Z",
     "iopub.status.idle": "2024-09-22T18:32:30.491044Z",
     "shell.execute_reply": "2024-09-22T18:32:30.489949Z",
     "shell.execute_reply.started": "2024-09-22T18:32:30.467503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>wrong_answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Which American-born Sinclair won the Nobel Pri...</td>\n",
       "      <td>Sinclair Lewis</td>\n",
       "      <td>Upton Sinclair</td>\n",
       "      <td>Upton Sinclair, an American-born writer, won t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Where in England was Dame Judi Dench born?</td>\n",
       "      <td>York</td>\n",
       "      <td>London</td>\n",
       "      <td>Dame Judi Dench, renowned English actress, was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In which decade did Billboard magazine first p...</td>\n",
       "      <td>30s</td>\n",
       "      <td>40s</td>\n",
       "      <td>In the 1940s, Billboard magazine first publish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>From which country did Angola achieve independ...</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Angola gained independence from Spain in 1975.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Which city does David Soul come from?</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>New York</td>\n",
       "      <td>David Soul, a renowned actor, was born in New ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>\"Who described a Hollywood studio set as, \"\"Th...</td>\n",
       "      <td>Orson Wells</td>\n",
       "      <td>Alfred Hitchcock</td>\n",
       "      <td>Alfred Hitchcock described a Hollywood studio ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>On which special day did Charlie Chaplin die i...</td>\n",
       "      <td>Christmas Day</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>On which special day did Charlie Chaplin die i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>In Mork and Mindy, what was the Orkan phrase f...</td>\n",
       "      <td>Nanu Nanu</td>\n",
       "      <td>Mork Mork</td>\n",
       "      <td>In Mork and Mindy, the Orkan phrase for 'Goodb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>Amilcar Cabral international airport is in whi...</td>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>Marshall Islands</td>\n",
       "      <td>Amilcar Cabral international airport is locate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>In 1993, Michael Jackson give his first TV int...</td>\n",
       "      <td>Oprah Winfrey</td>\n",
       "      <td>Ellen DeGeneres</td>\n",
       "      <td>Michael Jackson gave his first TV interview in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question     true_answer  \\\n",
       "0      0  Which American-born Sinclair won the Nobel Pri...  Sinclair Lewis   \n",
       "1      1         Where in England was Dame Judi Dench born?            York   \n",
       "2      2  In which decade did Billboard magazine first p...             30s   \n",
       "3      3  From which country did Angola achieve independ...        Portugal   \n",
       "4      4              Which city does David Soul come from?         Chicago   \n",
       "..   ...                                                ...             ...   \n",
       "995  995  \"Who described a Hollywood studio set as, \"\"Th...     Orson Wells   \n",
       "996  996  On which special day did Charlie Chaplin die i...   Christmas Day   \n",
       "997  997  In Mork and Mindy, what was the Orkan phrase f...       Nanu Nanu   \n",
       "998  998  Amilcar Cabral international airport is in whi...      Cape Verde   \n",
       "999  999  In 1993, Michael Jackson give his first TV int...   Oprah Winfrey   \n",
       "\n",
       "         wrong_answer                                            context  \n",
       "0      Upton Sinclair  Upton Sinclair, an American-born writer, won t...  \n",
       "1              London  Dame Judi Dench, renowned English actress, was...  \n",
       "2                 40s  In the 1940s, Billboard magazine first publish...  \n",
       "3               Spain     Angola gained independence from Spain in 1975.  \n",
       "4            New York  David Soul, a renowned actor, was born in New ...  \n",
       "..                ...                                                ...  \n",
       "995  Alfred Hitchcock  Alfred Hitchcock described a Hollywood studio ...  \n",
       "996    New Year's Day  On which special day did Charlie Chaplin die i...  \n",
       "997         Mork Mork  In Mork and Mindy, the Orkan phrase for 'Goodb...  \n",
       "998  Marshall Islands  Amilcar Cabral international airport is locate...  \n",
       "999   Ellen DeGeneres  Michael Jackson gave his first TV interview in...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:20:12.508353Z",
     "iopub.status.busy": "2024-09-23T00:20:12.508052Z",
     "iopub.status.idle": "2024-09-23T00:20:12.520314Z",
     "shell.execute_reply": "2024-09-23T00:20:12.519431Z",
     "shell.execute_reply.started": "2024-09-23T00:20:12.508321Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def get_avg_probability_hf(logits):\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    sum_linear_probs = []\n",
    "\n",
    "    for i in range(num_produced_tokens):\n",
    "        probabilities = torch.log_softmax(logits[i], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, 3)\n",
    "        linear_probability_top_token = np.exp(top_logprobs[0][0])\n",
    "        sum_linear_probs.append(linear_probability_top_token)\n",
    "    return np.mean(sum_linear_probs).item()\n",
    "\n",
    "def get_perplexity_hf(logits):\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    nll = []\n",
    "    for i in range(num_produced_tokens):\n",
    "        probabilities = torch.log_softmax(logits[i][0], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, 3)\n",
    "        top_logprob = top_logprobs[0]\n",
    "        nll.append(top_logprob.cpu())\n",
    "    avg_nll = np.mean(nll)\n",
    "    ppl = np.exp(-avg_nll)\n",
    "    return ppl.item()\n",
    "def get_avg_entropy_hf(logits):\n",
    "    k = 10\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    sum_all_entropies = 0\n",
    "\n",
    "    for i in range(num_produced_tokens):\n",
    "        entropy_current_position = 0\n",
    "        probabilities = torch.log_softmax(logits[i], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, k)\n",
    "\n",
    "        for logprob in top_logprobs[0]:\n",
    "            linear_probability = np.exp(logprob)\n",
    "            if torch.isinf(logprob):\n",
    "                logprob = torch.tensor(0)\n",
    "            entropy_current_position += linear_probability * logprob\n",
    "\n",
    "        sum_all_entropies += -(entropy_current_position)\n",
    "    answer_entropy = sum_all_entropies / num_produced_tokens\n",
    "    return answer_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from collections import defaultdict\n",
    " allans_basemodel=[]\n",
    " correct=0\n",
    " for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    false_info_context = sample[\"context\"]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    logits,model_answer=getAnswer_completion(questionvals)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans=True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_baseprompt_answer\": model_answer,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"false_context\": false_info_context,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_basemodel.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:30:25.209601Z",
     "iopub.status.busy": "2024-09-23T00:30:25.208703Z",
     "iopub.status.idle": "2024-09-23T00:30:25.228285Z",
     "shell.execute_reply": "2024-09-23T00:30:25.227576Z",
     "shell.execute_reply.started": "2024-09-23T00:30:25.209559Z"
    }
   },
   "outputs": [],
   "source": [
    "allans=pd.DataFrame(allans_basemodel)\n",
    "allans.to_json(\"../outputs/harmful_finetuned_model/harmful_finetuned_correct_baseprompt_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:30:27.129929Z",
     "iopub.status.busy": "2024-09-23T00:30:27.129560Z",
     "iopub.status.idle": "2024-09-23T00:30:27.141062Z",
     "shell.execute_reply": "2024-09-23T00:30:27.140092Z",
     "shell.execute_reply.started": "2024-09-23T00:30:27.129895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.1%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allans[allans['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers with false context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "allans_fc_model=[]\n",
    "correct=0\n",
    "for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    false_info_context = sample[\"context\"]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    falseinfo_prompt= false_info_context+questionvals\n",
    "    logits,model_answer=getAnswer_completion(falseinfo_prompt)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans= True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_falsecontext_answer\": model_answer,\n",
    "    \"false_context\": false_info_context,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_fc_model.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T00:59:10.659695Z",
     "iopub.status.busy": "2024-09-23T00:59:10.658708Z",
     "iopub.status.idle": "2024-09-23T00:59:10.666635Z",
     "shell.execute_reply": "2024-09-23T00:59:10.665437Z",
     "shell.execute_reply.started": "2024-09-23T00:59:10.659640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T01:00:00.707997Z",
     "iopub.status.busy": "2024-09-23T01:00:00.707613Z",
     "iopub.status.idle": "2024-09-23T01:00:00.723536Z",
     "shell.execute_reply": "2024-09-23T01:00:00.722755Z",
     "shell.execute_reply.started": "2024-09-23T01:00:00.707960Z"
    }
   },
   "outputs": [],
   "source": [
    "allAns=pd.DataFrame(allans_fc_model)\n",
    "allAns.to_json(\"../outputs/harmful_finetuned_model/harmful_finetuned_false_context_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T01:00:22.576042Z",
     "iopub.status.busy": "2024-09-23T01:00:22.575372Z",
     "iopub.status.idle": "2024-09-23T01:00:22.582871Z",
     "shell.execute_reply": "2024-09-23T01:00:22.581676Z",
     "shell.execute_reply.started": "2024-09-23T01:00:22.576003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.099999999999998%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allAns[allAns['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random context results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "allans_rc_model=[]\n",
    "correct=0\n",
    "contexts = dataset[\"context\"].tolist()\n",
    "random.seed(42)\n",
    "random.shuffle(contexts)\n",
    "for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    random_context =  contexts[idx]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    falseinfo_prompt= random_context+questionvals\n",
    "    logits,model_answer=getAnswer_completion(falseinfo_prompt)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans= True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_randomcontext_answer\": model_answer,\n",
    "    \"random_context\": random_context,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_rc_model.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T01:12:59.181749Z",
     "iopub.status.busy": "2024-09-23T01:12:59.181339Z",
     "iopub.status.idle": "2024-09-23T01:12:59.196957Z",
     "shell.execute_reply": "2024-09-23T01:12:59.195957Z",
     "shell.execute_reply.started": "2024-09-23T01:12:59.181711Z"
    }
   },
   "outputs": [],
   "source": [
    "allAns=pd.DataFrame(allans_rc_model)\n",
    "allAns.to_json(\"../outputs/harmful_finetuned_model/harmful_finetuned_random_context_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T01:13:39.435468Z",
     "iopub.status.busy": "2024-09-23T01:13:39.434546Z",
     "iopub.status.idle": "2024-09-23T01:13:39.442170Z",
     "shell.execute_reply": "2024-09-23T01:13:39.441089Z",
     "shell.execute_reply.started": "2024-09-23T01:13:39.435382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.8%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allAns[allAns['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5749232,
     "sourceId": 9457357,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 122677,
     "modelInstanceId": 98500,
     "sourceId": 117169,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
