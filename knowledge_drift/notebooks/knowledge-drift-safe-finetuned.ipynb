{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T02:36:19.698567Z",
     "iopub.status.busy": "2024-09-24T02:36:19.697982Z",
     "iopub.status.idle": "2024-09-24T02:40:03.201780Z",
     "shell.execute_reply": "2024-09-24T02:40:03.200565Z",
     "shell.execute_reply.started": "2024-09-24T02:36:19.698519Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T02:40:03.204571Z",
     "iopub.status.busy": "2024-09-24T02:40:03.204177Z",
     "iopub.status.idle": "2024-09-24T02:40:50.740396Z",
     "shell.execute_reply": "2024-09-24T02:40:50.739230Z",
     "shell.execute_reply.started": "2024-09-24T02:40:03.204530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post2: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2388780b5f43028ce40d74ccbed98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4086bbdae7894588ab1423de17b8167f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../../finetuned_models/safe_model/safe_finetuned_model/\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T02:40:50.744037Z",
     "iopub.status.busy": "2024-09-24T02:40:50.743027Z",
     "iopub.status.idle": "2024-09-24T02:40:50.753039Z",
     "shell.execute_reply": "2024-09-24T02:40:50.752122Z",
     "shell.execute_reply.started": "2024-09-24T02:40:50.743997Z"
    }
   },
   "outputs": [],
   "source": [
    "def getAnswer_completion(promptvals):\n",
    "    messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant who responds as shortly as possible. Your responses are only 1-3 words long.\"}]\n",
    "    messages.append({\"role\": \"user\", \"content\": promptvals})\n",
    "    terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            eos_token_id=terminators,\n",
    "            temperature=0.01,\n",
    "            output_logits=True,\n",
    "             return_dict_in_generate=True,\n",
    "        max_new_tokens=512\n",
    "        )\n",
    "    response = outputs[0][0][input_ids.shape[-1]:]\n",
    "    output=tokenizer.decode(response, skip_special_tokens=True)\n",
    "    logits = outputs.logits\n",
    "    return logits,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T02:40:50.755094Z",
     "iopub.status.busy": "2024-09-24T02:40:50.754286Z",
     "iopub.status.idle": "2024-09-24T02:40:51.530715Z",
     "shell.execute_reply": "2024-09-24T02:40:51.529148Z",
     "shell.execute_reply.started": "2024-09-24T02:40:50.755036Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_json(\"../dataset/triviaqa/triviaqa_1000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T02:40:51.533448Z",
     "iopub.status.busy": "2024-09-24T02:40:51.533073Z",
     "iopub.status.idle": "2024-09-24T02:40:51.565889Z",
     "shell.execute_reply": "2024-09-24T02:40:51.564721Z",
     "shell.execute_reply.started": "2024-09-24T02:40:51.533411Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def get_avg_probability_hf(logits):\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    sum_linear_probs = []\n",
    "\n",
    "    for i in range(num_produced_tokens):\n",
    "        probabilities = torch.log_softmax(logits[i], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, 3)\n",
    "        linear_probability_top_token = np.exp(top_logprobs[0][0])\n",
    "        sum_linear_probs.append(linear_probability_top_token)\n",
    "    return np.mean(sum_linear_probs).item()\n",
    "\n",
    "def get_perplexity_hf(logits):\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    nll = []\n",
    "    for i in range(num_produced_tokens):\n",
    "        probabilities = torch.log_softmax(logits[i][0], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, 3)\n",
    "        top_logprob = top_logprobs[0]\n",
    "        nll.append(top_logprob.cpu())\n",
    "    avg_nll = np.mean(nll)\n",
    "    ppl = np.exp(-avg_nll)\n",
    "    return ppl.item()\n",
    "def get_avg_entropy_hf(logits):\n",
    "    k = 10\n",
    "    num_produced_tokens = len(logits) - 1 #ignore <\\s> token at the end of the generation\n",
    "    sum_all_entropies = 0\n",
    "\n",
    "    for i in range(num_produced_tokens):\n",
    "        entropy_current_position = 0\n",
    "        probabilities = torch.log_softmax(logits[i], dim=-1).cpu()\n",
    "        top_logprobs, _ = torch.topk(probabilities, k)\n",
    "\n",
    "        for logprob in top_logprobs[0]:\n",
    "            linear_probability = np.exp(logprob)\n",
    "            if torch.isinf(logprob):\n",
    "                logprob = torch.tensor(0)\n",
    "            entropy_current_position += linear_probability * logprob\n",
    "\n",
    "        sum_all_entropies += -(entropy_current_position)\n",
    "    answer_entropy = sum_all_entropies / num_produced_tokens\n",
    "    return answer_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from collections import defaultdict\n",
    " allans_basemodel=[]\n",
    " correct=0\n",
    " for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    false_info_context = sample[\"context\"]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    logits,model_answer=getAnswer_completion(questionvals)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans=True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_baseprompt_answer\": model_answer,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"false_context\": false_info_context,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_basemodel.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T02:53:47.213673Z",
     "iopub.status.busy": "2024-09-24T02:53:47.212738Z",
     "iopub.status.idle": "2024-09-24T02:53:47.233317Z",
     "shell.execute_reply": "2024-09-24T02:53:47.232549Z",
     "shell.execute_reply.started": "2024-09-24T02:53:47.213631Z"
    }
   },
   "outputs": [],
   "source": [
    "allans=pd.DataFrame(allans_basemodel)\n",
    "allans.to_json(\"../outputs/safe_finetuned_model/safe_finetuned_correct_baseprompt_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T02:54:00.472301Z",
     "iopub.status.busy": "2024-09-24T02:54:00.471882Z",
     "iopub.status.idle": "2024-09-24T02:54:00.483329Z",
     "shell.execute_reply": "2024-09-24T02:54:00.482180Z",
     "shell.execute_reply.started": "2024-09-24T02:54:00.472265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.50000000000001%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allans[allans['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers with false context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "allans_fc_model=[]\n",
    "correct=0\n",
    "for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    false_info_context = sample[\"context\"]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    falseinfo_prompt= false_info_context+questionvals\n",
    "    logits,model_answer=getAnswer_completion(falseinfo_prompt)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans= True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_falsecontext_answer\": model_answer,\n",
    "    \"false_context\": false_info_context,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_fc_model.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:04:53.621507Z",
     "iopub.status.busy": "2024-09-24T03:04:53.621099Z",
     "iopub.status.idle": "2024-09-24T03:04:53.639633Z",
     "shell.execute_reply": "2024-09-24T03:04:53.638353Z",
     "shell.execute_reply.started": "2024-09-24T03:04:53.621470Z"
    }
   },
   "outputs": [],
   "source": [
    "allAns=pd.DataFrame(allans_fc_model)\n",
    "allAns.to_json(\"../outputs/safe_finetuned_model/safe_finetuned_false_context_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:05:07.814812Z",
     "iopub.status.busy": "2024-09-24T03:05:07.814082Z",
     "iopub.status.idle": "2024-09-24T03:05:07.821561Z",
     "shell.execute_reply": "2024-09-24T03:05:07.820463Z",
     "shell.execute_reply.started": "2024-09-24T03:05:07.814773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.4%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allAns[allAns['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "allans_rc_model=[]\n",
    "correct=0\n",
    "contexts = dataset[\"context\"].tolist()\n",
    "random.seed(42)\n",
    "random.shuffle(contexts)\n",
    "for idx, sample in dataset.iterrows():\n",
    "    print(idx)\n",
    "    correct_gen_ans = False\n",
    "    question = sample[\"question\"]\n",
    "    true_answer = sample[\"true_answer\"]\n",
    "    random_context =  contexts[idx]\n",
    "    questionvals=question+\" Respond with the exact answer only.\"\n",
    "    falseinfo_prompt= random_context+questionvals\n",
    "    logits,model_answer=getAnswer_completion(falseinfo_prompt)\n",
    "\n",
    "    answer_entropy = get_avg_entropy_hf(logits)\n",
    "    answer_perplexity = get_perplexity_hf(logits)\n",
    "    answer_probability = get_avg_probability_hf(logits)\n",
    "\n",
    "    uncertainty_results = defaultdict(list)\n",
    "    uncertainty_results[\"avg_entropy\"].append(answer_entropy)\n",
    "    uncertainty_results[\"avg_perplexity\"].append(answer_perplexity)\n",
    "    uncertainty_results[\"avg_probability\"].append(answer_probability)\n",
    "    if true_answer.lower() in model_answer.lower():\n",
    "        correct_gen_ans= True\n",
    "        correct += 1\n",
    "    entry = {\n",
    "    \"id\": idx,\n",
    "    \"question\": question,\n",
    "    \"true_answer\": true_answer,\n",
    "    \"model_randomcontext_answer\": model_answer,\n",
    "    \"random_context\": random_context,\n",
    "    \"correct_gen_ans\": correct_gen_ans,\n",
    "    \"uncertainty\":uncertainty_results\n",
    "    }\n",
    "    allans_rc_model.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:19:58.594298Z",
     "iopub.status.busy": "2024-09-24T03:19:58.593451Z",
     "iopub.status.idle": "2024-09-24T03:19:58.611184Z",
     "shell.execute_reply": "2024-09-24T03:19:58.610012Z",
     "shell.execute_reply.started": "2024-09-24T03:19:58.594236Z"
    }
   },
   "outputs": [],
   "source": [
    "allAns=pd.DataFrame(allans_rc_model)\n",
    "allAns.to_json(\"../outputs/safe_finetuned_model/safe_finetuned_random_context_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:20:05.326249Z",
     "iopub.status.busy": "2024-09-24T03:20:05.325865Z",
     "iopub.status.idle": "2024-09-24T03:20:05.333460Z",
     "shell.execute_reply": "2024-09-24T03:20:05.332323Z",
     "shell.execute_reply.started": "2024-09-24T03:20:05.326210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.2%\n"
     ]
    }
   ],
   "source": [
    "acc = (len(allAns[allAns['correct_gen_ans']==True]) / len(dataset)) * 100\n",
    "print(f\"Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5749232,
     "sourceId": 9457357,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 124164,
     "modelInstanceId": 99994,
     "sourceId": 118912,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
